{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "决策树分类原理小结",
   "id": "f77a4bdce23913fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1、常见决策树的启发函数比较",
   "id": "1bff944e2d87a91c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    （1）信息熵\n",
    "![jupyter](../Sources/Pictures/Decision_Tree_Algorithm/总结-信息熵.png)"
   ],
   "id": "3e073f036673665d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    （2）信息增益——ID3决策树\n",
    "![jupyter](../Sources/Pictures/Decision_Tree_Algorithm/总结-ID3决策树.png)"
   ],
   "id": "cf280ba737958481"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    （3）信息增益率——C4.5决策树\n",
    "![jupyter](../Sources/Pictures/Decision_Tree_Algorithm/总结-C4.5决策树.png)"
   ],
   "id": "776059fc9072138c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    （4）基尼值\n",
    "![jupyter](../Sources/Pictures/Decision_Tree_Algorithm/总结-基尼值.png)"
   ],
   "id": "2c58770debf019db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    （5）基尼指数——CART\n",
    "![jupyter](../Sources/Pictures/Decision_Tree_Algorithm/总结-基尼指数CART决策树.png)"
   ],
   "id": "b64ab7bdf3e26eb2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![jupyter](../Sources/Pictures/Decision_Tree_Algorithm/汇总.png)",
   "id": "d3b219de5fac1af3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    1.1.ID3算法\n",
    "    存在的缺点\n",
    "        (1) ID3算法在选择根节点和各内部节点中的分支属性时，采用信息增益作为评价标准。\n",
    "        信息增益的缺点是倾向于选择取值较多的属性，在有些情况下这类属性可能不会提供太多\n",
    "        有价值的信息.\n",
    "        (2) ID3算法只能对描述属性为离散型属性的数据集构造决策树。\n",
    "    1.2.C4.5算法\n",
    "        做出的改进(为什么使用C4.5要好)\n",
    "            (1) 用信息增益率来选择属性\n",
    "            (2) 可以处理连续数值型属性\n",
    "            (3)采用了一种后剪枝方法\n",
    "            (4)对于缺失值的处理\n",
    "\n",
    "        C4.5算法的优缺点\n",
    "            优点：产生的分类规则易于理解，准确率较高。\n",
    "            缺点：\n",
    "                在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法\n",
    "                的低效。   \n",
    "                此外，C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳\n",
    "                时程序无法运行。\n",
    "    1.3.CART算法\n",
    "        CART算法相比C4.5算法的分类方法，采用了简化的二叉树模型，同时特征选择采用了近\n",
    "        似的基尼系数来简化计算。\n",
    "        C4.5不一定是二叉树，但CART一定是二叉树。\n",
    "    1.4.多变量决策树(multi-variate decision tree)\n",
    "        同时，无论是ID3, C4.5还是CART,在做特征选择的时候都是选择最优的一个特征来做分\n",
    "        类决策，但是大多数，分类决策不应该是由某一个特征决定的，而是应该由一组特征决定\n",
    "        的。这样决策得到的决策树更加准确。这个决策树叫做多变量决策树(multi-variate \n",
    "        decision tree)。在选择最优特征的时候，多变量决策树不是选择某一个最优特征，而\n",
    "        是选择最优的一个特征线性组合来做决策。这个算法的代表是OC1，这里不多介绍。\n",
    "        如果样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习里面的\n",
    "        随机森林之类的方法解决。"
   ],
   "id": "fd08b008a31fd8a6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    2.决策树变量的两种类型：\n",
    "        （1）数字型（Numeric）：变量类型是整数或浮点数，如前面例子中的“年收入”。用\n",
    "            “>=”，“>”,“<”或“<=”作为分割条件（排序后，利用已有的分割情况，可以优化分割算法的时间复杂度）。\n",
    "        （2）名称型（Nominal）：类似编程语言中的枚举类型，变量只能从有限的选项中选取\n",
    "            ，比如前面例子中的“婚姻情况”，只能是“单身”，“已婚”或“离婚”，使用“=”来分割。"
   ],
   "id": "dbdbcd628ed02be6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    3.如何评估分割点的好坏？\n",
    "    如果一个分割点可以将当前的所有节点分为两类，使得每一类都很“纯”，也就是同一类的记录较\n",
    "    多，那么就是一个好分割点。\n",
    "    比如上面的例子，“拥有房产”，可以将记录分成了两类，“是”的节点全部都可以偿还债务，非\n",
    "    常“纯”；“否”的节点，可以偿还贷款和无法偿还贷款的人都有，不是很“纯”，但是两个节点加\n",
    "    起来的纯度之和与原始节点的纯度之差最大，所以按照这种方法分割。\n",
    "    构建决策树采用贪心算法，只考虑当前纯度差最大的情况作为分割点。"
   ],
   "id": "43c132069c1d910f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
