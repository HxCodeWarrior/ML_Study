{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "回归决策树",
   "id": "d47741d089d6d7a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    前面我们已经讲到，关于数据类型，我们主要可以把其分为两类，连续型数据和离散型数据\n",
    "    在面对不同数据时，决策树也可以分为两大类：\n",
    "        分类决策树，主要用于处理离散型数据\n",
    "        回归决策树，主要用于处理连续型数据"
   ],
   "id": "44da9ce9f2554224"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    不管是回归决策树，还是分类决策树，都会存在两个核心问题：\n",
    "        如何选择划分结点？\n",
    "        如何决定叶节点的输出值？"
   ],
   "id": "50e995c5352af80a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1.原理概述",
   "id": "bfaf662946a1a3b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    一个回归树对应着输入空间（即特征空间）的一个划分以及在划分单元上的输出值。\n",
    "    分类树中，我们采用信息论中的方法，通过计算选择器最佳化分点\n",
    "    而在回归树中，采用的是启发式的方法。假如我们有n个特征，每个特征有si(i∈()1,n)个取值\n",
    "    ，那我们就遍历所有特征，尝试该特征所有取值，对空间进行划分，直到取到特征j的取值s，使\n",
    "    得损失函数最小，这样就得到了一个划分点。描述该过程的公式如下：\n",
    "![jupyter](../Sources/Pictures/Decision_Tree_Algorithm/回归决策树原理公式.png)"
   ],
   "id": "faf3a7e4c9b56fd5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    假设将输入控件划分为M个单元：R1，R2，……，Rm那么每个区域的输出值就是：cm=avg(yi|zi∈Rm)\n",
    "    也就是该区域内所有点y值的平均数\n",
    "    举例：\n",
    "        如下图，加入我们想要对楼内的居民的年龄进行回归，\n",
    "        将楼划分为3个区域R1，R2，R3（红线），\n",
    "        那么R1的输出就是第一列四个居民年龄的平均值，\n",
    "        R2的输出就是第二列四个居民年龄的平均值，\n",
    "        R3的输出就是第三、四列八个居民年龄的平均值\n",
    "![jupyter](../Sources/Pictures/Decision_Tree_Algorithm/回归决策树案例-1.png)"
   ],
   "id": "bd59f7a91f981d8d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2.算法描述",
   "id": "31137049b2fac63c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    （1）输入：训练数据集D\n",
    "    （2）输出：回归树f(x)\n",
    "    （3）在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区 \n",
    "        域的输出值。\n",
    "    （4）构建二叉决策树："
   ],
   "id": "d5291614afc5def8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "        （4.1）选择最优切分特征j有切分点x，然后进行求解\n",
    "              遍历特征j，对固定的切分特征j扫描切分点，选择使得上式达到最小的对(j,s)\n",
    "![jupyter](../Sources/Pictures/Decision_Tree_Algorithm/回归决策树算法描述-1.png)"
   ],
   "id": "1beb97b719458049"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "        （4.2）用选定的对(j，s)划分区域并决定相应的输出值\n",
    "![jupyter](../Sources/Pictures/Decision_Tree_Algorithm/回归决策树算法描述-2.png)"
   ],
   "id": "815ac22fd94176ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "        （4.3）继续对两个字区域调用步骤（1）和（2），直至满足停止条件\n",
    "        （4.4）将输入空间划分为M个区域R1，R，……，RM。生成决策树：\n",
    "![jupyter](../Sources/Pictures/Decision_Tree_Algorithm/回归决策树算法描述-3.png)"
   ],
   "id": "30e1f7a66965ffdd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3.简单实例",
   "id": "a09f345027683d2e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    为了易于理解，接下来通过一个简单的实例加深对回归决策树的理解\n",
    "    训练数据见下表，目标得到一颗最小二乘回归树\n",
    "![jupyter](../Sources/Pictures/Decision_Tree_Algorithm/决策回归树案例-2.png)"
   ],
   "id": "1b919dddb1d82f07"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    3.1.实例计算过程\n",
    "        （1）选择最优的切分特征j与最优切分点s：\n",
    "            （1.1）确定第一个问题；选择最优切分点特征：\n",
    "                在本数据集中，只有一个特征，因此最优切分特征自然是x\n",
    "            （1.2）确定第二个问题：我们考虑9个切分点[1..5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5]\n",
    "                损失函数定义为平方损失函数Loss(y,f(x))=(f(x)-y)^2，\n",
    "                将上述9个切分点依此带入下面的公式中，其中cm=avg(yi|xi∈Rm)\n",
    "        a、计算区域输出值：\n",
    "            例如，取s=1.5，此时R1=1，R2=2，3，4，5，6，7，8，9，10，这两个区域\n",
    "            的输出值分别为：\n",
    "                c1=5.56\n",
    "                c2=(5.7+5.91+6.4+6.8+7.05+8.9+8.7+9+9.05)/9=7.50\n",
    "            同理，得到其他各切分点的子区域输出值，如下表\n",
    "![jupyter](../Sources/Pictures/Decision_Tree_Algorithm/决策树回归案例2数据分析-a.png)"
   ],
   "id": "398f66a1dfefa17"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "            b、计算损失函数值，找到最优切分点\n",
    "            把c1、c2的值带入到同平方损失函数Loss(y,f(x))=(f(x)-y)^2\n",
    "            当s=1.5时，\n",
    "            L(1.5)=(5.56-5.56)^2+[(5.7-7.5)^2+(5.91+7.5)^2+……+(9.05-7.5)^2]=0+15.72=15.72\n",
    "            同理，计算得到其他各切分点的损失函数值，可获得下表：\n",
    "            显然取s=6.5，m(s)最小。因此，第一个划分变量[j=x,s=6.5]\n",
    "![jupyter](../Sources/Pictures/Decision_Tree_Algorithm/决策树回归案例2数据分析-b.png)"
   ],
   "id": "383e84866c2c1139"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "        （2）用选定的（j，s）划分区域，并决定输出值：\n",
    "            两个区域分别为：R1={1，2，3，4，5，6}，R2={7，8，9，，10}\n",
    "            输出值cm=avg(yi|xi∈Rm)，c1=6.24，c2=8.91\n",
    "        （3）调用步骤（1）、（2）继续划分：\n",
    "            对R1继续进行划分：\n",
    "![jupyter](../Sources/Pictures/Decision_Tree_Algorithm/决策树案例分析2-31.png)"
   ],
   "id": "47b7f2673f13250"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "            取切分点[1.5,2.5,3.5,4.5,5.5]，则各区域的输出值c如下表：\n",
    "![jupyter](../Sources/Pictures/Decision_Tree_Algorithm/决策树案例分析2-32.png)"
   ],
   "id": "79d659279efed05b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "            计算损失函数值m（s）：\n",
    "            可以知道：s=3.5时，损失函数值m（s）最小\n",
    "![jupyter](../Sources/Pictures/Decision_Tree_Algorithm/决策树案例分析2-33.png)"
   ],
   "id": "c760bdac2fd1e59a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "        (4)生成回归树\n",
    "        假设在生成3个区域之后停止划分，那么最终生成的回归树形式如下：\n",
    "![jupyter](../Sources/Pictures/Decision_Tree_Algorithm/回归决策树案例分析2-4.png)"
   ],
   "id": "da32c59052a89716"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
