{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "cart剪枝",
   "id": "d35bbab7d56254e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1.为什么要剪枝",
   "id": "8c449a1c73f6b858"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![jupyter](../Sources/Pictures/Decision_Tree_Algorithm/cart剪枝效果图.png)",
   "id": "f9dcf23a29582435"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    (1)图形描述\n",
    "        （1.1）横轴表示在决策树创建过程中树的节点总数，纵轴表示决策树的预测精度\n",
    "        （1.2）实线表示决策树在训练集上的精度，虚线显示的是在一个独立的测试集上测量\n",
    "              出来的结果。\n",
    "        （1.3）随着树的增长，在训练样本集上的精度是单调上升的。然而在独立的测试样本\n",
    "              集上测出的精度先上升后下降。\n",
    "    （2）原因分析：\n",
    "        （2.1）原因1：噪声、样本冲突，即错误的样本数据\n",
    "        （2.2）原因2：特征即属性不能完全作为分类标准\n",
    "        （2.3）原因3：巧合地规律性，数据量不够大"
   ],
   "id": "b6e7440ce9466cca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    剪枝（pruning）是决策树学习算法对付“过拟合”的主要手段\n",
    "    在决策树学习中，为了尽可能正确分类训练样本，结点划分过程将不断重复，有时会造成决策\n",
    "    树分支过多，这是就可能因训练样本学习“太好”，以至于把训练集自身的一些特点也当作所有\n",
    "    数据都具有的一般性质而导致过拟合。因此，可通过主动去掉一些分支来降低过拟合的风险。"
   ],
   "id": "5ef50c5d4ce5f31a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    如何判断决策树泛化性能是否提升？\n",
    "    可以使用前面介绍的留出法，即预留出一部分数据用作“验证集”以进行性能评估。例如下表的\n",
    "    西瓜数据集，我们将其随即划分为两部分。其中编号为（1，2，3，6，7，10，14，15，16\n",
    "    ，17）的样例组成训练集，编号为（4，5，8，9，11，12，13）的样例组成验证集。\n",
    "![jupyter](../Sources/Pictures/Decision_Tree_Algorithm/cart剪枝西瓜案例数据.png)"
   ],
   "id": "407c7c2ad60a1562"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    假定我们采用信息增益准则来划分属性选择，则上表中训练集将会生成一颗下面的据册数，为\n",
    "    了便于讨论，我们对圈中的部分结点做了编号。\n",
    "![jupyter](../Sources/Pictures/Decision_Tree_Algorithm/剪枝西瓜案例决策树.png)    "
   ],
   "id": "f85e8c5ad73aa6e3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2.常用的剪枝方法",
   "id": "4f0c7dceb1adaa48"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    决策树剪枝的基本策略有“预剪枝”（ore-pruning）和“后剪枝”（post-pruning）\n",
    "        预剪枝：是指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分\n",
    "        不能带来决策树泛化性能提升，则停止划分并将当前节点标记为叶节点。\n",
    "        后剪枝：事先从训练集生成一颗完整的决策树，然后自底向上地对非叶节点进行考察，若\n",
    "        将该节点对应地子树替换为叶节点能带来决策树泛化性能的提升，则将该子树替换为叶节点。"
   ],
   "id": "5d0f5afd4beee190"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    2.1 预剪枝",
   "id": "54d38b33d5200263"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "        首先，基于信息增益准则，我们回选取属性“脐部”来对训练集进行划分，并产生3个分\n",
    "        支，如下图所示。然而，是否应该进行这个划分呢？预剪枝要对划分前后地泛化性能进行评估。\n",
    "![jupyter](../Sources/Pictures/Decision_Tree_Algorithm/预剪枝西瓜案例.png)"
   ],
   "id": "b4d231e26d395112"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "        在划分之前，所有样例集中在根节点。\n",
    "            若不进行划分，该节点将被标记为叶节点，其类别标记为训练样例数最多的类别，\n",
    "            假设我们将这个叶节点标记为“好瓜”\n",
    "            用前面表的验证集对这个单节点决策树进行评估。则编号为（4，5，8）的样例被\n",
    "            分类正确。另外4个样例分类错误，于是验证集精度为3/7 * 100% = 42.9%\n",
    "        在用属性“脐部”划分之后，上图中的结点2，3，4分别包含编号为（1，2，3，14）、\n",
    "        （6，7，15，17）、（10，16）的训练样例，因此这3个结点分别被标记为叶节点“好\n",
    "        瓜”、“好瓜”、“怀瓜”\n",
    "        此时验证集中标号为（4，5，8，11，12）的样例被分类正确，\n",
    "        验证集精度为5/7 * 100% = 71.4% >42.9%\n",
    "分析![jupyter](../Sources/Pictures/Decision_Tree_Algorithm/预剪枝案例分析.png)"
   ],
   "id": "be317ecf082cc38e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "        于是用“脐部”进行划分确定。\n",
    "        然后决策树算法应该对结点2进行划分，基于信息增益准则将挑选出划分属性“色泽”，\n",
    "        然而，在使用“色泽”划分后，编号为（5）的验证集样本呢分类结果会有正确转换为错\n",
    "        误，使得验证集精度下降为57.1%。于是，预剪枝决策将禁止结点2被划分。\n",
    "        对于结点3，最优化分属性为“根蒂”，划分后验证集精度仍为71.4%，这个划分不能提\n",
    "        升验证集精度，于是预剪枝策略精致结点3被划分。\n",
    "        对于结点4，其所含训练样例已属于同一类，不再进行划分。\n",
    "        于是，基于预剪枝策略从上表数据所生成的决策树如上图所示，其验证集精度为71.4%\n",
    "        ，这是一颗仅有一层划分的决策树，亦称“决策树桩”（decision stump）"
   ],
   "id": "fa1fb6f3d426a36d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2.2 后剪枝",
   "id": "ec22a9c25386e02d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    后剪枝先从训练集生成一颗完整的据册数，继续使用上面的案例，从前面计算，我们知道前面\n",
    "    构造的决策树的验证集精度为42.9%\n",
    "![jupyter](../Sources/Pictures/Decision_Tree_Algorithm/后决策树案例.png)"
   ],
   "id": "23d1eb0e48605647"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    后剪枝首先考察结点6，若将其领衔的分支剪除则相当于把6替换为叶节点。替换后的叶结点包\n",
    "    含编号为（7，15）的训练样本，于是该叶结点的类别标记为“好瓜”，此时决策树的验证集精\n",
    "    度提高至57.1%。于是，后剪枝策略均订剪枝，如下图：\n",
    "![jupyter](../Sources/Pictures/Decision_Tree_Algorithm/后剪枝案例解析.png)"
   ],
   "id": "bd8beb01f4a1481f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    然后考察结点5，若将其领衔的子树替换为叶结点，则替换后的叶节点包含（6，7，15）的训\n",
    "    练样例，叶结点类别标记为“好瓜”；此时决策树验证集精度仍为57.1%，于是可以不进行剪枝。\n",
    "    对叶结点2，若将其领衔的子树替换为叶结点，则替换后的叶结点包含编号为(1，2，3，14)\n",
    "    的训练样例，叶结点标记为“好瓜”此时决策树的验证集精度提高至71.4%，于是，后剪枝策\n",
    "    略决定剪枝。\n",
    "    对叶结点3和1，若将其领衔的子树替换为叶结点，则所得决策树的验证集精度分别为71.4%\n",
    "    ，42.9%，均未得到提高，于是它们被保留。\n",
    "    最终，基于后剪枝策略所产生的决策树就如上图所示，其验证集精度为71.4%。"
   ],
   "id": "bbbd8f5d2d8b7390"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3.两种剪枝对比：",
   "id": "bababd3187ed6054"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    （1）后剪枝决策树通常比预剪枝决策树保留了更多的分支\n",
    "    （2）一般情形下，后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树\n",
    "    （3）但后剪枝过程是在生成完全决策树之后进行的。并且要自底向上地对树种的非叶结点进\n",
    "        行逐一考察，因此其训练实践开销比未剪枝决策树和预剪枝决策树都要大得多。"
   ],
   "id": "3443d27af0fe4b9b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4.小结",
   "id": "e43399e5949eec93"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    （1）剪枝原因【了解】\n",
    "        a、噪声、样本冲突，即错误的样本数据\n",
    "        b、特征即属性不能完全作为分类标准\n",
    "        c、巧合的规律性，数据量不够大\n",
    "    （2）常用剪枝方法\n",
    "        a、预剪枝\n",
    "            限制节点最小样本数\n",
    "            指定数据高度\n",
    "            指定熵值的最小值\n",
    "        b、后剪枝\n",
    "            把一颗树，构建完成后，再进行从下往上的剪枝"
   ],
   "id": "8d51e82c325216c3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
