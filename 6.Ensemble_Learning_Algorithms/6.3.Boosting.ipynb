{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Boosting",
   "id": "c0db4ff40613c11b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1.什么是Boosting",
   "id": "8b09522ad7dff780"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    随着学习的积累，从弱到强\n",
    "    简而言之：每新加入一个弱学习器，整体能力就会的得到提升\n",
    "    代表算法：Adaboost、GBDT、XGBoost、LightGBM\n",
    "![jupyter](../Sources/Pictures/Ensemble_Learning_Algorithms/Boosting动画展示.png)"
   ],
   "id": "d0d28bf91473a335"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2.实现过程",
   "id": "eabaa0508f2c4eb3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    2.1.训练第一个学习器\n",
    "![jupyter](../Sources/Pictures/Ensemble_Learning_Algorithms/Boosting实现流程-1.png)"
   ],
   "id": "b768ccfb647537b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    2.2.调整数据分布\n",
    "![jupyter](../Sources/Pictures/Ensemble_Learning_Algorithms/Boosting实现流程-2.png)"
   ],
   "id": "6471fb6de22da4d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    2.3.训练第二个学习器\n",
    "![jupyter](../Sources/Pictures/Ensemble_Learning_Algorithms/Boosting实现流程-3.png)"
   ],
   "id": "66ddd987a9e1e436"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    2.4.再次调整数据分布\n",
    "![jupyter](../Sources/Pictures/Ensemble_Learning_Algorithms/Boosting实现流程-4.png)"
   ],
   "id": "4f0000c53338c2ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    2.5.依次训练学习器，调整数据分布\n",
    "![jupyter](../Sources/Pictures/Ensemble_Learning_Algorithms/Boosting实现流程-5.png)"
   ],
   "id": "8a937943e4278c8e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    2.6.整体过程实现\n",
    "![jupyter](../Sources/Pictures/Ensemble_Learning_Algorithms/Boosting整体过程实现.png)"
   ],
   "id": "e70185d19a7eb19a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3.Bagging集成与Boosting集成的区别",
   "id": "569ed1952c86da95"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    3.1.数据方面\n",
    "        （1）Bagging：对数据进行采样训练\n",
    "        （2）Boosting：根据前一轮学习结果调整数据的重要性\n",
    "    3.2.投票方面\n",
    "        （1）Bagging：所有学习器平均投票\n",
    "        （2）Boosting：对学习器进行加权投票\n",
    "    3.3.学习顺序：\n",
    "        （1）Bagging：并行学习，每个学习器没有依赖关系\n",
    "        （2）Boosting：串行学习，学习有先后顺序\n",
    "    3.4.主要作用：\n",
    "        （1）Bagging：主要用于提高泛化性能（解决过拟合，也可以说降低方差）\n",
    "        （2）Boosting：主要用提提高训练精度（解决欠拟合，也可以说降低偏差）\n",
    "![jupyter](../Sources/Pictures/Ensemble_Learning_Algorithms/Bagging集成与Boosting集成的区别.png)"
   ],
   "id": "fad5b9ee75758001"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4.AdaBoost介绍",
   "id": "b83d212df23df04e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    4.1.构造过程细节\n",
    "        （1）步骤一：初始化训练数据，权重相等，训练第一个学习器\n",
    "                   该假设每个训练样本在基本分类器的学习过程中作用相同，这一假设\n",
    "                   可以保证第一步能够在原始数据商学系基本分类器H1(x)\n",
    "        （2）步骤二：AdaBoost反复学习基本分类器，在每一轮m=1，2，……，M顺次\n",
    "            执行下列操作：\n",
    "                （2.1）在权重分布为Dt的训练数据上，确定基分类器\n",
    "                （2.2）计算该学习器在训练数据中的错误率：  Σt = P(ht(xt)!=yt)\n",
    "                （2.3）计算该学习器的投票权重：   αt = ln((1-Σt) / Σt) / 2\n",
    "                （2.4）根据投票权重，对训练数据重新赋权（如下图）\n",
    "                       将下一轮学习器的注意力集中在错误数据上\n",
    "                （2.5）重复执行（2.1）到（2.4），m次\n",
    "![juppyter](../Sources/Pictures/Ensemble_Learning_Algorithms/AdaBoost重新赋权.png)"
   ],
   "id": "1f833b3a2c5c2138"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "        （3）步骤三：对m个学习器进行加权投票\n",
    "![jupyter](../Sources/Pictures/Ensemble_Learning_Algorithms/AdaBoost加权投票.png)"
   ],
   "id": "4cfaed7442fab9c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    4.2.关键点剖析\n",
    "        （1）如何确认投票权重？\n",
    "        （2）如何调整数据分布？\n",
    "![jupyter](../Sources/Pictures/Ensemble_Learning_Algorithms/Boosting实现关键点.png)"
   ],
   "id": "f54a5c259945558"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    4.3.案例\n",
    "        给定下面这张训练数据表所示数据，假设弱分类器xv产生，其阈值v使该分类器在训练数据集上的分类误差率最低，试用Adaboost算法学习一个强分类器。\n",
    "![jupyter](../Sources/Pictures/Ensemble_Learning_Algorithms/Adaboost案例数据.png)"
   ],
   "id": "963b5994e2cca9db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    步骤解答：\n",
    "        （1）步骤一：初始化训练数据权重相等，训练第一个学习器：\n",
    "                D1 = (w11, w12,……,w110)\n",
    "                w1i = 0.1, i=1,2,……,10\n",
    "        （2）步骤二：AdaBoost反复学习基本分类器，在每一轮m=1，2，……，M顺次的执行下列操作：\n",
    "            当m=1的时候：\n",
    "                （a）在权重分布为D1的训练数据上，阈值v取2.5时分类误差率最低，故基本分类器为：\n",
    "                    （此时6，7，8被分类错误）\n",
    "![jupyter](../Sources/Pictures/Ensemble_Learning_Algorithms/Adaboost案例解析2-1.png)"
   ],
   "id": "6fdb7563d55448ed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "                （b）计算该学习器在训练数据中的错误率：c1=P（h1（x1）！=Y1) = 0.3\n",
    "                （c）计算该学习器的投票权重: α1 = (ln((1-Σ1)/Σ1)) / 2 = 0.4236\n",
    "                （d）根据投票权重，对训练数据重新赋权：\n",
    "                    D2 = （w21，w22，……，w210）\n",
    "                    根据下公式，计算各个权重值\n",
    "![juppyter](../Sources/Pictures/Ensemble_Learning_Algorithms/AdaBoost重新赋权.png)"
   ],
   "id": "865987438d4e099"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "             经计算得D2的值为：\n",
    "             D2=(0.07143,0.07143,0.07143,0.07143,0.07143,0.07143,0.16667,0.16667,0.07143)\n",
    "             \n",
    "            计算过程（如下图）：\n",
    "            H1(x) = sign[0.4236h1(x)]\n",
    "            分类器H1(x)在训练数据集上有3个误分类点（6，7，8）\n",
    "![jupyter](../Sources/Pictures/Ensemble_Learning_Algorithms/AdaBoost案例计算过程.png)"
   ],
   "id": "223377d4e72a7715"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "        当m=2的时候：\n",
    "            （a）在权值分布为D2得训练数据上，阈值y取8.5时分类误差率最低，故基本分类器为：\n",
    "![jupyter](../Sources/Pictures/Ensemble_Learning_Algorithms/Adaboost案例解析2-2.png)"
   ],
   "id": "d2cde4ad040751e9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "            （b）计算该学习器在训练数据中得错误率：Σ2 = P(h2(x2) != y2) = 0.2143\n",
    "            （c）计算该学习器得投票权重：α2 = ln((1-Σ2)/Σ2) / 2 = 0.6496\n",
    "            （d）根据投票权重，对训练数据重新赋权：\n",
    "            经计算得，D3为：\n",
    "            D3=（0.0445，0.0445，0.0445，0.1667，0.1667，0.1667，0.1060，0.1060，0.1060，0，0455）\n",
    "            H2（x） = sign[0.42366h1(x) + 0.6496h2(x)]\n",
    "            分类器H2(x)在训练数据集上有3个误分类点"
   ],
   "id": "5c969f0ae7175419"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "        当m=3的时候：\n",
    "            （a）在权值分布为D2得训练数据上，阈值y取8.5时分类误差率最低，故基本分类器为：\n",
    "![jupyter](../Sources/Pictures/Ensemble_Learning_Algorithms/Adaboost案例解析2-3.png)"
   ],
   "id": "3dd52eb759f2e68f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "            （b）计算该学习器在训练数据中得错误率：Σ2 = P(h2(x2) != y2) = 0.1820\n",
    "            （c）计算该学习器得投票权重：α2 = ln((1-Σ2)/Σ2) / 2 = 0.7514\n",
    "            （d）根据投票权重，对训练数据重新赋权：\n",
    "            经计算得，D3为：\n",
    "            D3=（0.125，0.125，0.125，0.102，0.102，0.102，0.065，0.065，0.065，0.125）\n",
    "            H2（x） = sign[0.4236h1(x) + 0.3496h2(x) + 0.7514h3(x)]\n",
    "            分类器H3(x)在训练数据集上有0个误分类点"
   ],
   "id": "61161772fca1d5b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4.4.API介绍",
   "id": "8c0d9977647b2c57"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "[官方API链接](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier)"
   ],
   "id": "4e7a6ead7fb646fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "5.小结",
   "id": "377fbee263c4954d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    1.什么是Boosting【知道】\n",
    "        （1）随着学习的积累从弱到强\n",
    "        （2）代表算法：Adaboost、GBDT、XGBoost、LightGMB\n",
    "    2.bagging和boosting的区别【知道】\n",
    "        （1）区别一；数据方面：\n",
    "            Bagging：对数据进行采样训练\n",
    "            Boosting：根据前一轮学习结果调整数据的重要性\n",
    "        （2）区别二：投票方面：\n",
    "            Bagging：所有学习器平均投票\n",
    "            Boosting：对学习器进行加权投票\n",
    "        （3）区别三：学习顺序：\n",
    "            Bagging：并行学习，每个学习器没有依赖关系\n",
    "            Boosting：串行学习，学习有先后顺序\n",
    "        （4）区别四：主要作用；\n",
    "            Bagging：主要用于提高泛化性能（解决过拟合问题，也可以说降低方差）\n",
    "            Boosting：主要用于提高训练精度（解决欠拟合问题，也可以说降低偏差）\n",
    "    3.AdaBoost构造过程【知道】\n",
    "        （1）初始化训练数据权重相等，训练第一个学习器\n",
    "        （2）AdaBoost反复学习基本分类器\n",
    "        （3）对m个学习器进行加权投票"
   ],
   "id": "5df2f0e9eafba1a2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
