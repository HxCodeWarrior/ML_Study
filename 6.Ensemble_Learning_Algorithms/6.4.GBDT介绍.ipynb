{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "GBDT介绍",
   "id": "dd17fe189a3394d8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    GBDT的全称Gradient Boosting Decision Tree，梯度提升树，在传统机器学习算法中，\n",
    "    GBDT算的上TOP3的算法，想要理解GBDT的真正意义，就必须理解GBDT中的Gradient \n",
    "    Boosting和Decision Tree分别是什么？"
   ],
   "id": "22e35833129c2038"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1.Decision Tree：CART回归树",
   "id": "27deb1451f934fae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "        首先，GBDT使用的决策树是CART回归树，无论是处理回归问题还是二分类以及多分类问题，GBDT使用的决策树统统都是CART回归树。\n",
    "        为什么不用CART分类树呢？\n",
    "        ————因为GBDT每次迭代要你和的梯度值，是连续值所以要用回归树\n",
    "        对于回归树算法来说，最终要的是寻找最佳的划分点，那么回归树中的可划分点包含了所\n",
    "        有特征的所有可取的值。\n",
    "        在分类树中最佳化分店的判别标准是熵或者基尼系数，都是用纯度来衡量，但是在回归树\n",
    "        中的样本标签是连续数值，所i在使用熵之类的指标不再合适，取而代之的是平方误差，它\n",
    "        能很好的评判拟合程度。"
   ],
   "id": "8c8c1727e6ac4360"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    1.1回归树生成算法（复习）\n",
    "        输入：训练数据集D\n",
    "        输出：回归树f（x）\n",
    "        在训练数据集所在的输入控件中，递归的将每个区域划分为两个子区域并决定每个子区域\n",
    "        的输出值，构建二叉树"
   ],
   "id": "ba2f3bffe04baf7c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2.Gradient Boosting：拟合负梯度",
   "id": "5aa7cf06c38a6e74"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    梯度提升树（Gradient Boosting）是提升树（Boosting Tree）的一种改进算法，\n",
    "    所以在讲梯度提升树之前先来说说提升树。\n",
    "    先来一个通俗的理解：假如有个人30岁，我们首先用20岁去拟合，发现损失值10岁，这时我们\n",
    "    用6岁去你和剩下的损失，发现差距还有4岁，第三轮我们用3岁去拟合剩下的差距，差距只有1\n",
    "    岁。如果我们用迭代轮数还没有完。可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小\n",
    "    。最后将每次拟合的岁数加起来便是模型输出的结果。"
   ],
   "id": "2b7d772fabe865f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    提升树算法：\n",
    "        （1）初始化f0(x)\n",
    "        （2）对m=1，2，……，M\n",
    "            （2.1）计算残差rmi = yi - fm - l(x), i=1,2，……，N\n",
    "            （2.2）拟合残差rmi学习一个回归树，得到hm(x)\n",
    "            （2.3）更新fm(x) = fm-l + hm(x)\n",
    "        （3）得到回归问题提升树fM(x) = ∑(1->M)hm(x)"
   ],
   "id": "9120a772c535ddcf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    上述伪代码中的残差是什么？\n",
    "    在提升树算法中，\n",
    "        假设我们前一轮迭代得到的强学习器是：ft - 1(x)\n",
    "        损失函数为：L(y, ft - 1(x))\n",
    "        我们本轮迭代的目的是找到一个弱学习器：ht(x)\n",
    "        最小化本轮的损失：L(y, ft(x)) = L(y, ft-1(x)+ht(x))\n",
    "        当采用平方损失函数时：\n",
    "            L(y, ft-1(x)+ht(x)) = (y - ft-1 (x) - ht(x))^2\n",
    "                                = (r - ht(x))^2\n",
    "        这里 r = y - ft - 1(x) 是当前模型拟合数据的残差（residual）\n",
    "        所以，对于提升树来说只需要简单地拟合当前模型的残差\n",
    "    回到我们上面讲的那个通俗易懂的例子中，第一次迭代的残差是10岁，第二次残差4岁……"
   ],
   "id": "6bc0a0bea15c0f8e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    当损失函数是平方损失和指数损失函数时，梯度提升树每一步优化是很简单的，但是对于一般损\n",
    "    失函数而言，往往每一步优化起来不是那么容易。\n",
    "    针对这一问题，Friedman提出了梯度提升树算法，这是利用最速下降的近似方法，其关键是利\n",
    "    用损失函数的负梯度作为提升树算法中的残差的近似值。\n",
    "    那么负梯度是什么样？"
   ],
   "id": "8e40a9d4de3ab066"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    第一t轮的第i个样本的损失函数的负梯度为：\n",
    "![jupyter](../Sources/Pictures/Ensemble_Learning_Algorithms/负梯度-1.png)"
   ],
   "id": "b50f0dd1b3f9f024"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    此时不同的损失函数将会得到不同的负梯度，如果选择平方损失：\n",
    "![jupyter](../Sources/Pictures/Ensemble_Learning_Algorithms/负梯度-2.png)"
   ],
   "id": "27743575b0a72f8d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    负梯度为：\n",
    "![jupyter](../Sources/Pictures/Ensemble_Learning_Algorithms/负梯度-3.png)    "
   ],
   "id": "2231035cd7f3821"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    此时我们发现GBDT的负梯度就是残差，所以说遂于回归问题，我们要拟合的就是残差\n",
    "    那么对于分类问题呢？\n",
    "    二分类和多分类的损失函数都是logloss\n",
    "    本文已回归问题为例进行讲解"
   ],
   "id": "85fbed33df358c2c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.GBDT算法",
   "id": "12c6deebd1908a6e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    上面我们分别将Decision Tree和Gradient Boosting介绍完毕，下面见这两部分\n",
    "    组合在一起就是我们的GBDT算法了。"
   ],
   "id": "d2d2a2c4b46879c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# GBDT算法",
   "id": "d2077002b4aa503c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    （1）初始化弱学习器\n",
    "![jupyter](../Sources/Pictures/Ensemble_Learning_Algorithms/GBDT算法原理-1.png)"
   ],
   "id": "d93eb319332d7cd1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    （2）对m=1，2，……，M有：\n",
    "        （a）对每个样本i=1，2，……，N，计算负梯度，即残差：\n",
    "![jupyter](../Sources/Pictures/Ensemble_Learning_Algorithms/GBDT算法原理-21.png)"
   ],
   "id": "afd333d92283a137"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "        （b）将上步得到的残差作为样本新的真实值，并将数据（xi, rim），i=1，2，……N\n",
    "        作为下棵树的训练数据，得到一颗新的回归树fm(x)其对应的叶子节点区域为Rjm，\n",
    "        j=1,2,……，J。其中J为回归树t的叶子节点的个数。\n",
    "        （c）随叶子区域j=1，2，……，J计算最佳拟合值：\n",
    "![jupyter](../Sources/Pictures/Ensemble_Learning_Algorithms/GBDT算法原理-23.png)"
   ],
   "id": "f17d577f6f8f16b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "        （d）更新强学习器\n",
    "![jupyter](../Sources/Pictures/Ensemble_Learning_Algorithms/GBDT算法原理-24.png)"
   ],
   "id": "8f984e9607ad32e3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    （3）得到最终学习器\n",
    "![jupyter](../Sources/Pictures/Ensemble_Learning_Algorithms/GBDT算法原理-3.png)"
   ],
   "id": "14060ab01624b547"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
