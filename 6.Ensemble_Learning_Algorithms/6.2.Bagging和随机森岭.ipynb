{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Bagging和随机森林",
   "id": "8745dd2e3d9f1f2c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1.Bagging集成原理",
   "id": "3d7e79ff3064d62"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    目标：把下面的圆圈和方块进行分类\n",
    "![jupyter](../Sources/Pictures/Ensemble_Learning_Algorithms/Bagging集成原理图解-0.png)"
   ],
   "id": "b2e4b907c57b9eb8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    实现过程",
   "id": "4d524313f8cc8c40"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    （1）采样不同数据集\n",
    "![jupyter](../Sources/Pictures/Ensemble_Learning_Algorithms/Bagging集成原理图解-1.png)"
   ],
   "id": "a97f6558b995fbfd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    （2）训练分类器\n",
    "![jupyter](../Sources/Pictures/Ensemble_Learning_Algorithms/Bagging集成原理图解-2.png)"
   ],
   "id": "c220177a34220351"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    （3）平均投票，获取最终结果\n",
    "![jupyter](../Sources/Pictures/Ensemble_Learning_Algorithms/Bagging集成原理图解-3.png)"
   ],
   "id": "ecf6b7220f938f6e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    （4）主要实现过程小结\n",
    "![jupyter](../Sources/Pictures/Ensemble_Learning_Algorithms/Bagging集成原理图解-4.png)"
   ],
   "id": "c8cb6f517fe02064"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2.随机森林构造过程",
   "id": "88bab18ebb371b23"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    在机器学习中，随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出\n",
    "    的类别的众数而定。"
   ],
   "id": "14d614c1cbdcafc6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    随机森林 = Bagging + 决策树\n",
    "![jupyter](../Sources/Pictures/Ensemble_Learning_Algorithms/随机森林构造过程图解.png)"
   ],
   "id": "c5b7943c33001548"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    例如，如果你训练了5棵树，其中有4个树的结果是True，1个树的结果是False，那么最终投票就是True\n",
    "    随机森林构造过程中的关键步骤（M表示特征数目）：\n",
    "        （1）一次随机选出一个样本，有放回抽样，重复N次（有可能出现重复的样本）\n",
    "        （2）随机去选出m个特征，m<<M，建立决策树"
   ],
   "id": "5f72f53ddb79ff1d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    思考：\n",
    "        （1）为什么要随机抽样训练集？\n",
    "            如果不进行随机抽样，每棵树的训练集都是一样的，那么最终训练出的树分类结果\n",
    "            也是完全一样的。\n",
    "        （2）为什么要有放回抽样？\n",
    "            如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这\n",
    "            样每棵树都是“有偏的”，都是绝对“片面的”（当然这么说可能不对），也就是说\n",
    "            每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类\n",
    "            器）的投票表决。"
   ],
   "id": "c454d54319e16927"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3.包外估计（Out-of-Bag Estimate）",
   "id": "e7d1612184cdaef0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    在随机森林构造的过程中，如果进行有放回的抽样，我们会发现总有一部分样本我们是选不到的。\n",
    "    这部分数据，占整体数据的比重有多大？\n",
    "    这部分数据有什么用？"
   ],
   "id": "9fa16b5a1fadaa9a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "        3.1.包外估计定义\n",
    "        随机森林的Bagging过程，对于每一颗训练出的决策树gt，与数据集D有如下关系：\n",
    "![jupyter](../Sources/Pictures/Ensemble_Learning_Algorithms/包外估计定义.png)"
   ],
   "id": "3b6c26783cb75bdc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "        对于星号的部分，即是没有选择到的数据，称之为Out-of-Bag（OOB）数据，当数据\n",
    "        足够多的时候，对于任意一组数据（xn，yn）是包外数据的概率：\n",
    "![jupyter](../Sources/Pictures/Ensemble_Learning_Algorithms/包外数据概率计算.png)"
   ],
   "id": "f8c0cf2aa2f9f9c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "        由于基分类器是构建在训练样本的自助抽样集上，只有约63.2%原样本集出现在其中，\n",
    "        而剩余36.8%的数据作为包外数据，可以用于基分类器的验证集。\n",
    "        经验证：包外数据是对集成分类器泛化误差的无偏估计\n",
    "        在随机森林算法中数据属性的重要性、分类器集强度和分类器间相关性计算都依赖于包外数据。\n",
    "[什么是无偏估计](Expand/拓展1：无偏估计.ipynb)"
   ],
   "id": "1a8ca08317c940b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "        3.2.包外估计的用途\n",
    "            （1）当基学习器是决策树时，可使用包外样本来辅助剪枝，或用于估计决策树中\n",
    "            各结点的后验概率以辅助对零训练样本结点的处理。\n",
    "            （2）当基学习器是神经网络时，可使用包外样本来辅助早期停止以减小过拟合。"
   ],
   "id": "54f364d0d523aa3f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4.随机森林API介绍",
   "id": "7e19418180041cdb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    sklearn.ensemble.RandomForestClassifier(\n",
    "        n_estimators=10,criterion=\"gini\",\n",
    "        max_depth=None,Bootstrap=True,\n",
    "        random_state=None,mini_samples_split=2\n",
    "    )"
   ],
   "id": "abe524f134fa498f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    参数详解：\n",
    "    n_estimator：类型：integer\n",
    "                 可选，默认为10\n",
    "                 森林里的数目数量：120，200，300，500，800，1200\n",
    "    Criterion：类型：string\n",
    "               可选，默认为gini\n",
    "               作用：分割特征的测量方法\n",
    "    max_depth:类型：integer或者None\n",
    "              可选，默认为None\n",
    "              树的最大深度5，8，15，25，30\n",
    "    max_features:类型：string，每个决策树的最大特征数量\n",
    "                 可选，默认为auto\n",
    "                 if “auto”,then:   max_features=sqrt(n_features)\n",
    "                 if \"sqrt\",then:   max_features=sqrt(n_features)(same as \"auto\")\n",
    "                 if \"log2\",then:   max_features=log2(n_features)\n",
    "                 if None,then:     max_features=n_features\n",
    "    bootstrap:类型：boolean\n",
    "              可选，默认为True\n",
    "              作用：是否在构建树时使用有放回抽样\n",
    "    min_samples_split:内部节点再划分所需最少样本数\n",
    "            可选，默认值为2\n",
    "            作用：这个值限制了子树继续划分的条件，\n",
    "                 如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分\n",
    "                 如果样本量不大，不需要管这个值。如果样本量数级非常大，则推荐增大这个值\n",
    "    min_samples_leaf:叶子节点的最小样本数\n",
    "            可选，默认值为1\n",
    "            作用：这个值限制了叶子节点最少的样本数\n",
    "                 如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝\n",
    "            注意：叶子是决策树的末端节点，较小的叶子使得模型更容易波卓训练数据中的噪声\n",
    "                一般来说，偏向于将最小叶子节点数目设置为大于50\n",
    "    min_impurity_spllit:节点划分最小不纯度\n",
    "            作用：这个值限制了决策树的增长，如果某节点的不纯度（即基尼系数，均方差）\n",
    "                 小于这值，则该节点不再生成子节点，即为叶子节点。\n",
    "            注意：一般不推荐改动默认值1e-7\n",
    "    超参数：\n",
    "        n_estimator,max_depth,min_samples_split,min_samples_leaf\n",
    "    \n",
    "    最重要的参数：\n",
    "        最大特征数：max_features\n",
    "        最大深度：max_depth\n",
    "        内部节点再划分所需要最小样本数：min_samples_split\n",
    "        叶子节点最小样本数：min_samples_leaf"
   ],
   "id": "672708b768319e22"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    注意：随机森林的建立过程\n",
    "         树的深度，数的个数等需要进行超参数调优"
   ],
   "id": "c3d1c7423fcde09e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "5.Bagging集成优点",
   "id": "945228662980f87e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    Bagging + 决策树/线性回归/逻辑回归/深度学习…… = Bagging集成学习方法\n",
    "    \n",
    "    经过上面方式组成的集成学习方法：\n",
    "        （1）均可在原有算法上提高约2%左右的泛化正确率\n",
    "        （2）简单、方便、通用"
   ],
   "id": "8b9483d65b24ab5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "6.小结",
   "id": "2590e12a205d12df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    1、Bagging集成过程【知道】\n",
    "        （1）采样--从所有样本里，采样一部分\n",
    "        （2）学习--训练弱学习器\n",
    "        （3）集成--使用平均投票\n",
    "    2、随机森林介绍【知道】\n",
    "        （1）随机森林定义：随机森林=Bagging+决策树\n",
    "        （2）流程：\n",
    "            （2.1）随机选取m条数据\n",
    "            （2.2）随机选取k个特征\n",
    "            （2.3）训练决策树\n",
    "            （2.4）重复1~3\n",
    "            （2.5）对上面的若决策树进行平均投票\n",
    "        （3）注意：\n",
    "            （3.1）随机选取样本呢，且是有放回的抽样\n",
    "            （3.2）选取特征的时候，选择m << M\n",
    "            （3.3）M是所有的特征数\n",
    "        （4）包外估计\n",
    "            如果进行有放回的对数据集抽样，会发现，总有一部分样本选不到\n",
    "        （5）API\n",
    "            sklearn.ensemble.RandomForestClassifier()\n",
    "    3、Bagging + 决策树/线性回归/逻辑回归/深度学习…… = Bagging集成学习方法\n",
    "    4、Bagging的优点【了解】\n",
    "        （1）均可在原有算法上提高约2%左右的泛化正确率\n",
    "        （2）简单、方便、通用"
   ],
   "id": "8e26b0cf31c9c03f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
