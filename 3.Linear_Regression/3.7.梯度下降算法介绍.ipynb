{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "详解梯度下降算法",
   "id": "292cfeef15036626"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    1.梯度下下降的相关概念复习\n",
    "        （1）步长（Learning rate）：\n",
    "            步长决定了在梯度下降迭代的过程中，每一步沿梯度负方向前进的长度。用前面下山\n",
    "            的例子，步长就是在当前这一步所在位置沿着最陡峭最易下山的位置走的那一步的长度\n",
    "        （2）特征（feature）\n",
    "            指的是样本中输入部分，比如2个单特征的样本(x^(0),y^(0),(x^(1),y^(1)),则第一个样本特征为x(0),第一个样本的输出为y(0)\n",
    "        （3）假设函数（hypothesis function）\n",
    "            在监督学习中，为了拟合输入样本，而使用的假设函数，记为hθ(x)，比如对于单\n",
    "            个特征的m个样本（x^(i),y^(i)）(i=1,2,3,……)可以采用拟合函数如下：\n",
    "            hθ(x) = θ0 + θ1\n",
    "        （4）损失函数（loss function）\n",
    "            为了评估模型拟合的好坏程度，通常用损失函数来度量拟合的程度。损失函数极小\n",
    "            化，意味着拟合程度最好，对应的模型参数即为最优参数。\n",
    "            在线性回归中，损失函数常为样本输出和假设函数的差取平方，比如对于m个样本\n",
    "            （x^(i),y^(i)）(i=1,2,3,……)，采用线性回归，损失函数为：\n",
    "            注意：\n",
    "                xi表示第i个样本特征，\n",
    "                yi表示第i个样本对应的输出，\n",
    "                hθ(xi)为假设函数\n",
    "![jupyter](../Sources/Pictures/Linear_Regression/再讲梯度下降损失函数.png)"
   ],
   "id": "370c98f3b53c5cd5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2.梯度下降法的推导流程",
   "id": "ed356b5c608ca360"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "        （1）先决条件：确认优化模型的假设函数和损失函数\n",
    "            比如：对于线性回归，\n",
    "            假设函数表示为hθ(x1,x2,……,xn)=θ0 + θ1x1 + …… + θnxn，\n",
    "            其中:θi(i = 0,1,2,3,……)为模型参数，\n",
    "                xi(x=0,1,2,……，n)为\n",
    "                每个样本的n个特征值，\n",
    "            这表示可以简化，我们增加一个特征x0=1，这样：\n",
    "![jupyter](../Sources/Pictures/Linear_Regression/梯度下降算法推导-1.png)"
   ],
   "id": "4a135fae8ac02f99"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "        同样是是线性回归，对于上面的假设函数，损失函数为：\n",
    "![jupyter](../Sources/Pictures/Linear_Regression/梯度下降算法推导-2.png)"
   ],
   "id": "aa3c1154f0e0358f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "        （2）算法相关参数初始化\n",
    "            主要是初始化：θ0，θ1,……，θn\n",
    "            算法终止距离：Σ\n",
    "            步长：α\n",
    "        注意：在没有任何先验知识的时候，通常将所有的θ初始化为0，将步长初始化为1\n",
    "             ，在调优的时候再优化。"
   ],
   "id": "d900a8440ba39e03"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    （3）算法过程",
   "id": "134e94d5a980b38c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "        （3.1）确定当前位置的损失函数梯度，对于θi，其梯度表达式如下\n",
    "![jupyter](../Sources/Pictures/Linear_Regression/梯度下降算法推导-3.png)"
   ],
   "id": "e86ecd83117be497"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "        （3.2）用步长乘以损失函数的梯度，得到当前位置下降的距离，即：\n",
    "![jupyter](../Sources/Pictures/Linear_Regression/梯度下降算法推导-4.png)"
   ],
   "id": "ef438daa6c500e2b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "        （3.3）确定是都所有的θi，梯度下降的距离都小于Σ，如果小于Σ则算法终止，当前所\n",
    "              有的θi（i=0，1，2，……，n）即为最终结果，否则进入步骤4"
   ],
   "id": "3d7986a3fcd16511"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "        （3.4）更新所有的θi，其更新表达式如下，更新完毕后继续转入步骤1\n",
    "![jupyter](../Sources/Pictures/Linear_Regression/梯度下降算法推导-5.png)"
   ],
   "id": "bbca774db16ffc07"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    下面用线性回归的例子来具体描述梯度下降，假设我们的样本是如下图：\n",
    "    其中：x1，x2，……xn为特征值，y0，y，……，ym为目标值\n",
    "![jupyter](../Sources/Pictures/Linear_Regression/梯度下降算法推导举例-1.png)"
   ],
   "id": "fc4322698571f2c8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    损失函数如前面先决条件所描述：\n",
    "![jupyter](../Sources/Pictures/Linear_Regression/梯度下降算法推导举例-2.png)"
   ],
   "id": "97613aff62de8118"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    则在算法过程步骤1中对于θi的偏导数计算如下：\n",
    "![jupyter](../Sources/Pictures/Linear_Regression/梯度下降算法推导举例-3.png)"
   ],
   "id": "7fd16e20a1fb0b81"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    由于样本中没有x0上式中令所有的x0为1\n",
    "    步骤4中θi的更新表达式如下：\n",
    "![jupyter](../Sources/Pictures/Linear_Regression/梯度下降算法推导举例-4.png)"
   ],
   "id": "35c7c3d644a69987"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    从这个例子中可以看出当前点的梯度方向是由所有的样本巨顶的，加1/m是为了更好理解，他们的乘积也为常数，所以这里α*（1/m）可以用一个常数表示",
   "id": "8a85c4d169c68095"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3.梯度下降算法大家族",
   "id": "83f14da721c5e9e3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    首先我们来看一下，长江的梯度下降算法：\n",
    "        全梯度下降算法（Full gradient descent）\n",
    "        随机梯度下降算法（Stochastic gradient descent）\n",
    "        小批量梯度下降算法（Mini-batch gradient descent）\n",
    "        随机平均梯度下降算法（Stochastic average gradient descent）\n",
    "    它们都是为了正确地体哦阿姐权重向量，通过每个权重计算一个梯度，从而更新权值，使目标\n",
    "    函数尽可能最小化，其差别在于样本的使用方式不同。"
   ],
   "id": "e6f3a4e378c47671"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    3.1.全梯度下降算法（FG）\n",
    "    批量梯度下降算法，是梯度下降算法中最常用的形式，具体做法也就是在更新参数是使用所有的样本来进行更新。计算训练集所有样本误差，对其求和再取平均值作为目标函数\n",
    "    权重向量沿其梯度相反的方向上移动，从而使当前目标函数减少得最多。\n",
    "    其实在整个训练数据集上计算损失函数关于参数θ的梯度：\n",
    "![jupyter](../Sources/Pictures/Linear_Regression/全梯度下降算法.png)"
   ],
   "id": "3b96f550e6dc5d5b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    由于我们有m个样本，这里求梯度的时候就用了所有m个样本得梯度数据\n",
    "    注意：\n",
    "        a、因为在执行每次更新的时候，我们需要在整个数据集上计算所有的梯度，所以梯度下\n",
    "           降算法的速度会很慢，同时，批量梯度下降算法无法处理超出内存容量限制的数据集\n",
    "        b、批量梯度下降算法同样也不能在线更新模型，即在运行的过程中，不能增加新的样本"
   ],
   "id": "c47415ea912d8866"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    3.2.随机梯度下降算法（SG）\n",
    "        由于FG每次迭代更新一次权重都需要计算所有样本误差，而实际问题中经常有上亿的训\n",
    "        练样本。所以效率很低，且容量容易陷入局部最优解，因此提出了随机梯度下降算法。\n",
    "        \n",
    "        其每轮计算分目标函数不再是全体样本误差，而仅仅式单个样本误差，即每次值代入计\n",
    "        算一个样本目标函数的梯度类更新权重，再取下一个样本重复至此过程，知道损失函数\n",
    "        停止下降或损失函数值小于某个可以容忍的阈值。\n",
    "        \n",
    "        此过程简单、高效，同城可以较好地避免更新迭代收敛到局部量最优解，其迭代形式：\n",
    "![jupyter](../Sources/Pictures/Linear_Regression/随机梯度下降算法.png)"
   ],
   "id": "f17c7357f3e82fee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    3.3.小批量梯度下降算法（mini-batch）\n",
    "    小批量梯度下降算法式FG和SG的折中方案，在一定程度上兼顾了以上两种方法的优点。\n",
    "    每次从训练样本集上随机抽取一个小样本集，在抽出来的小样本集上采用FG迭代更新权重。\n",
    "    被抽出来的小样本集所含样本点的个数称为batch_size，通常设置为2的幂次方，更利于GPU加速处理。\n",
    "    特别的，若batch_size=1,则变成了SG；若batch_size=n，则变成了FG，\n",
    "    其迭代形式如下：\n",
    "    解释：我们从m个样本中，选择x个样本进行迭代（1<x<m）\n",
    "![jupyter](../Sources/Pictures/Linear_Regression/小批量梯度下降算法.png)"
   ],
   "id": "a151441f9a79525b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    3.4.随机平均梯度下降算法（SAG）\n",
    "    在SG方法中，虽然避开了运算成本大的问题，但对于大数据训练而言，SG效果常不尽人意，\n",
    "    因为每一轮梯度更新都完全与上一轮的数据和梯度无关。\n",
    "    随机平均梯度算法克服了这个问题，在内存中为每个样本都维护一个旧的梯度，随机选择第i\n",
    "    个样本来更新此样本的梯度，其他样本的梯度保持不变，然后求得所有梯度的平均值，进而更\n",
    "    新了参数。\n",
    "    如此，每一轮更新仅需要计算一个样本的梯度，计算成本等同于SG，三收敛速度快得多。\n",
    "    其迭代形式为：\n",
    "![jupyter](../Sources/Pictures/Linear_Regression/随机平均梯度下降算法.png)"
   ],
   "id": "32d0da6dad041451"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "        （1）我们知道agd是当前权重减去步长乘以梯度，得到新的权重，sag中的a就是平均的意思，具体说，就是在第k步迭代的时候，在考虑的这一步和前面n-1个梯度的平均值，当前权重减去步长乘以最近n个梯度的平均值。\n",
    "        （2）n是自己设置的，当n=1的时候，就是普通的sgd\n",
    "        （3）这个想法非常简单，在随机中又增加了确定性，类似于mini-batch sgd的作用，但不同的是，sag又没有去计算更多的样本，只是利用了之前计算出来的梯度，所以每次迭代的计算成本远小于mini-batch agd，和agd相当。效果而言，sag相对于agd，收敛速度要快的很多。这一点下面的论文中有具体的描述和证明。\n",
    "        （4）拓展：\n",
    "[SAG论文链接](http://arxiv.org/pdf/1309.2388.pdf)"
   ],
   "id": "319ae05210e4dbf2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
