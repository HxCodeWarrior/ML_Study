{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "梯度下降（Gradient Descent）",
   "id": "170e965c2e1aadb6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    1.什么是梯度下降\n",
    "    梯度下降法的基本思路可以理解为一个下山的过程。\n",
    "    假设有这样一个场景：\n",
    "        一个人被困在山上，需要从山上下来（找到山的最低点，也就是山谷）。但是此时山上的\n",
    "        浓雾很大，导致可视度很低。\n",
    "        因此，下山的路径就无法确定，他必须利用子集周围的信息去找到下山的路径，这个时候\n",
    "        ，他就可以利用梯度下降算法来帮助自己下山。\n",
    "        具体来说就是，以他当前的所处位置为基准，寻找这个位置最陡峭的地方，然后朝着山的\n",
    "        高度下降的地方走（同理，如果我们的目标是上山，也就是爬到山顶，那么此时应该是朝\n",
    "        着最陡峭的方向向上走）。然后每走一段距离，都反复采用同一个方法，最后就能成功抵\n",
    "        达山谷。\n",
    "    梯度下降的基本过程就和下山的场景很类似：\n",
    "            首先，我们有一个可微分的函数，这个函数就代表着一座山.我们的目标就是找到这 \n",
    "            个函数的最小值，也就是山底。\n",
    "            根据之前的场景假设，最快的下山方式就是找到挡墙最陡峭的方向，然后沿着此方向\n",
    "            向下走，对应到函数中，就是找到给定点的梯度，然后朝着梯度相反的方向，就能让\n",
    "            函数值下降的最快！因为梯度的方向就是函数值变化最快的方向。所以，我们重复利\n",
    "            用这个方法，反复求取梯度，最后就能抵达局部的最小值，这就类似于我们下山的过\n",
    "            程，而求取梯度就确定了最陡峭的方向，也就是场景中测量方向的手段。\n",
    "![jupyter](../Sources/Pictures/Linear_Regression/梯度下降与下山.png)"
   ],
   "id": "c4c83bc67ad280f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    2.梯度的概念\n",
    "        梯度是微积分中很重要的一个概念\n",
    "        （1）在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率\n",
    "        （2）在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点\n",
    "        的上升最快的方向。\n",
    "    这也就说明了为什么我们要千方百计的求取梯度！我们需要到达山底，就需要在每一步观测到此\n",
    "    时是最陡峭的地方，梯度就恰巧告诉了我们这个方向。梯度的方向是函数在给定点上升最快的方\n",
    "    向，那么梯度的反方向就是函数在给定点下降最快的方向。这个正是我们所需要的。所以我们只\n",
    "    要沿着梯度的反方向一直走，就能走到局部的最低点。"
   ],
   "id": "da4ff3e322297c83"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    3.梯度下降举例\n",
    "        （1）单变量函数的梯度下降（切线）\n",
    "            我们假设有一个单变量的函数：J(θ) = θ^2\n",
    "            函数的微分：J'(θ) = 2*θ\n",
    "            初始化起点：θ^0 = 1\n",
    "            学习率：α = 0.4\n",
    "            我们开始进行梯度下降的迭代计算的过程：\n",
    "![jupyter](../Sources/Pictures/Linear_Regression/单变量梯度下降迭代计算.png)"
   ],
   "id": "82b03935409a5697"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "        如图，经过四次的运算，也就是走了四步，基本就抵达了函数的最低点，，也就是山底.\n",
    "![jupyter](../Sources/Pictures/Linear_Regression/单变量梯度下降计算图解.png)"
   ],
   "id": "a1edf093ce7cd155"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    3.梯度下降举例\n",
    "        （2）多变量函数的梯度下降（向量）\n",
    "        假设有一个目标函数：J(θ) = θ1^2 + θ2^2\n",
    "        现在要通过梯度下降法计算这个函数的最小值，我们通过观察就能发现最小值其实就是（0，0）点。但是接下来，我们会从梯度下降算法开始一步步计算得到这个最小值\n",
    "        假设初始起点:θ^0 = (1,3)\n",
    "        初始的学习率：α = 0.1\n",
    "        函数的梯度：∇:J(θ) = <2*θ1, 2*θ2>\n",
    "        进行多次迭代：\n",
    "![jupyter](../Sources/Pictures/Linear_Regression/多变量梯度下降迭代计算.png)"
   ],
   "id": "1f7891c93a4ba8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    我们发现，已经基本靠近函数的最小点：\n",
    "![jupyter](../Sources/Pictures/Linear_Regression/多变量梯度下降计算图解.png)"
   ],
   "id": "548e06d82a37962c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    4.梯度下降公式\n",
    "![jupyter](../Sources/Pictures/Linear_Regression/梯度下降公式.png)"
   ],
   "id": "e4ec91da090e94df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    （1）α含义：\n",
    "        α在梯度下降算法中被称作为学习率或者步长，意味着我们可以通过α来控制每一步走的距离，以保证不要步子跨的太大，走得太快，错过了最低点。同时也要保证不要走得太慢，导致需要走很多很多步，计算量大耗费时间长。\n",
    "        所以，α的选择在梯度下降短发中往往很重要！！\n",
    "        α不能太大也不能太小，太小的话就可能导致迟迟走不到最低点；太大的话会导致错过最低点。\n",
    "![jupyter](../Sources/Pictures/Linear_Regression/梯度下降公式α含义.png)"
   ],
   "id": "86055e6836c5d277"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    （2）为什么梯度要乘以一个符号？\n",
    "    梯度前加上一个负号，就意味着朝着梯度相反的方向前进！我们在前文提到，梯度的方向实际\n",
    "    就是函数在此点上升最快的方向，而我们需要的是朝着下降最快的方向走，自然就是负的梯度\n",
    "    方向，所以此处需要加上符号。\n",
    "![jupyter](../Sources/Pictures/Linear_Regression/梯度下降负号理解.png)"
   ],
   "id": "6fc618a8ad5c9c29"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    我们通过两个图来更好理解梯度下降的过程：\n",
    "![jupyter](../Sources/Pictures/Linear_Regression/梯度下降理解.png)"
   ],
   "id": "9be300b9a81ad886"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
