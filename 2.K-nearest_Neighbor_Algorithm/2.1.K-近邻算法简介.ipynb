{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "    基础简介\n",
    "    k近邻算法（k-Nearest Neighbor，kNN）是机器学习所有算法中理论最简单、最好理解的\n",
    " 算法。它是一种基本的分类与回归方法，它的输入为实例的特征向量，通过计算新数据与训练数据\n",
    " 特征值之间的距离，然后选取k（k>=1）个举例最近的邻居进行分类判断（投票法）或者回归。如\n",
    " 果k=1，那么新数据被简单地分配给其近邻的类。\n",
    "    对于分类问题：输出为实例的类别。发呢类是，对于新的实例，根据其k个最近邻的训练实例，\n",
    " 类别，通过多数表决等方式进行预测。\n",
    "    对于回归问题：输出为实例的值。回归时，对于新的实例，取其k个最近邻的是训练实例的平均\n",
    " 值为预测值。\n",
    "    k近邻算法分类的直观理解：给定一个训练数据集，对于新地输入实例，在训练集中找到于该实\n",
    " 例最邻近的k个实例。这k个实例的多数属于某个类别，则该输入实例就划分为这个类别。\n",
    "    k近邻算法不具有显式的学习过程，它是直接进行预测。实际上它是利用训练数据集对特征向量\n",
    " 空间进行划分，并且作为其分类的“模型”。"
   ],
   "id": "5efc6102fa1604cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    1.1 K-近邻算法（KNN）概念\n",
    "        （1）定义：如果一个样本在特征空间中的k个最相似（即特征空间中最邻近）的样本中的\n",
    "        大多数属于某一个类别，则该样本也属于该类别\n",
    "        （2）距离公式\n",
    "            两个样本的距离可以通过如下公式计，又称欧式距离，关于距离公式会在后面进行讨论。\n",
    "            （2.1）二维平面上的点a(x1,y1)与b(x2,y2)间的欧式距离：\n",
    "                d(a->b)=((x1 - x2)^2 + (y1 - y2)^2)^0.5\n",
    "            （2.2）三维空间中点a(x1,y1,z1)与b(x2,y2,z2)间的欧式距离：\n",
    "                d(a->b)=((x1 - x2)^2 + (y1 - y2)^2 + (z1 - z2)^2)^0.5\n",
    "            （2.3）n维空间中两个点间a(x11,x12,……)与b(x21,x22,……)的欧式距离：\n",
    "                d(a->b)=(∑(x1k - x2k)^2) ^ 0.5"
   ],
   "id": "5fd8fa6a0d217d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    1.2 KNN算法流程\n",
    "        （1）计算已知类别数据集中的点与当前点之间的距离\n",
    "        （2）按距离递增次序排列\n",
    "        （3）选取与当前点距离最小的K个点\n",
    "        （4）统计前k个点所在的类别出现的频率\n",
    "        （5）返回前k个点出现频率最高的类别作为当前点的预测分类"
   ],
   "id": "7a645bbb855b2632"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    1.3 KNN算法三要素\n",
    "        k-近邻算法的三要素：距离、度量和分类决策规则（取均值的决策规则）\n",
    "        \n",
    "        1.3.1 K值选择\n",
    "            当k=1时的k-近邻算法称为最近邻算法。此时将训练集中与x最近的点的类别作为x的类\n",
    "            ！！！k值的选择会对k-近邻算法结果产生重大影响\n",
    "            （1）如果k值较小，则相当于用较小的领域中的训练实例进行预测，“学习”的近似误差减小\n",
    "                * 优点：只有与输入实例较近的训练实例才会对预测起作用\n",
    "                * 缺点：“学习”的估计误差会增大，预测结果会对近邻的实例点非常敏感。\n",
    "                        如果近邻的训练实例点刚好是噪声，则预测会出错。\n",
    "                        即：k值的减小，意味着模型整体变复杂，易发生过拟合。\n",
    "                        \n",
    "            （2）如果k值较大则相当于用较大的领域中的训练实例进行预测\n",
    "                * 优点：减小学习的估计误差\n",
    "                * 缺点：学习的近似误差会增大。\n",
    "                       这时输入实例较远的训练实例也会对预测起作用，使预测发生错误，\n",
    "                       即：k值增意味着模型整体变简单。\n",
    "                       当K=N时，无论输入实例是什么，都将它预测为训练实例中最多的一类\n",
    "                       （即预测结果是一个常量）。此时模型过于简单，完全忽略了训练实力中大量有用的信息。\n",
    "            \n",
    "            在应用中，k值一般去一个较小的值。通常采用交叉验证法来选取最优的k值，就是比\n",
    "            较不同的k值时的交叉验证平均误差率，选额误差率最小的那个k值。\n",
    "            例如：选择k=1，2，……，对每个k=i做若干次交叉验证，计算出平均误差，然后比\n",
    "            较，选出误差最小的那个。\n",
    "        \n",
    "        1.3.2 距离度量\n",
    "            （1）k-近邻算法要求数据的所有特征都可以做比较的量化。\n",
    "            如果在数据特征中存在非数值类型的数据，必须采取手段将其量化。\n",
    "            如果样本中有多个参数，每一个参数都有自己的定义域和取值范围，它们对距离计算\n",
    "            的影响也不一样，如果取值较大的影响力会盖过取值较小的参数。为了公平，样本参\n",
    "            数必须做一些归一化处理，最简单的方式就是所有特征的数值都采取归一化处理。\n",
    "            \n",
    "            例如：如果样本特征中包含性别（男、女）一项，男女之间没有距离可言，可以\n",
    "                 通过将男女转换为二维数组：男[1,0]\\女[0,1]来实现距离计算。\n",
    "            例如：一个样本中，性别只有男女，年龄有0-100，体重有50-100kg，身高有\n",
    "            100-200cm，四个特征值的取值范围相差都很大，,但是可以试图将这四个特征量都\n",
    "            转换到[0,1]或者[-1,1]的区间进行计算。\n",
    "            （2）特征空间中两个实例点的距离是两个实例点相似程度的反映。\n",
    "            k-近邻模型的特征空间一般是n维实数向量空间R^n。k近邻模型的特征空间的距离一般为欧式距离，也可以是一般的L（p）距离：\n",
    "                        L(x1, x2) = ( ∑|x1 - x2|^p ) ^ (1 / p)    (p>=1)\n",
    "                        \n",
    "            (3)几种特殊距离计算公式：\n",
    "                * 欧氏距离(p=2)\n",
    "                    L(x1, x2) = ( ∑|x1 - x2|^2 ) ^ 0.5\n",
    "                * 曼哈顿距离(p=1)\n",
    "                    L(x1, x2) = ∑|x1 - x2| \n",
    "                * 切比雪夫距离(p=∞)\n",
    "                    L(x1, x2) = max|x1 - x2|\n",
    "            \n",
    "            (4)不同的距离度量所确定的最近邻点是不同的。一般情况下，选择欧式距离了作为距离度量，到那时这只适用于连续变量。\n",
    "            在文本分类这种非连续变量情况下，汉明距离可以用来作为度量。\n",
    "            通常情况下，如果运用一些特殊算法来计算度量，那么k-近邻分类的进度可以显著提高，如果运用大边缘最近邻或者近邻成分分析法。\n",
    "            \n",
    "        1.3.3 分类决策规则\n",
    "            （1）分类决策通常采用多数表决，也可以基于距离的远近进行加权投票，距离越近的样本权重越大。\n",
    "            （2）多数表决规则等价于经验风险最小化。\n",
    "            设  分类的损失函数为0-1损失函数，\n",
    "                分类函数为f：R^n -> {c1,c2,……，ck}，\n",
    "                误分类概率为：P(Y != f(X))= 1-P(Y=f(X))\n",
    "            给定实例x∈γ，其最近邻的k个训练点构成集合Nk(x)。\n",
    "            设涵盖Nk(x)区域的类别为cj（这是待求的未知量，但它肯定是c1，c2，……ck之一），则误分类率为：\n",
    "                (1 / k) ∑(x∈Nk) I(yi != cj) = 1-(1 / k) ∑(x∈Nk) I(yi = cj)\n",
    "            （3）误分类率就是训练数据的经验风险。要使误分类率最小，即经验风险最小，就要使得∑(x∈Nk) I(yi != cj)最大。即多数表决：\n",
    "                cj = arg(cj)max ∑(x∈Nk) I(yi = cj)"
   ],
   "id": "f489e4b434fd3e74"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
