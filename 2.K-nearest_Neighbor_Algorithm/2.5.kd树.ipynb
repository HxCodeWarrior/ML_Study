{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "kd树",
   "id": "2417309d9ff60515"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    问题导入：\n",
    "        实现k近邻算法时，主要考虑的问题是如何对训练数据继续宁快速k近邻搜索。\n",
    "        这个特征空间的维数大以及训练数据容量大时尤其必要。\n",
    "        k近邻算法最简单拿的实现是线性扫描（穷举搜索），即要计算输入实例与每一个训练实例\n",
    "        的距离，计算并存储好以后，再查找k近邻。当训练集相当大时，计算非常耗时。\n",
    "        为了提高kNN搜索的效率，可以考虑使用特殊的结构存储训练数据，以减小计算距离的次数"
   ],
   "id": "d18c7c381378721a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    1.kd树简介\n",
    "        1.1 什么是kd树\n",
    "            （1）引入：根据kNN每次需要预测一个点时，我们都需要计算训练数据集里每个点\n",
    "            到这个点的距离，然后选出距离最近的k个点进行投票，当数据集很大时，这个计算\n",
    "            成本非常高。\n",
    "            （2）kd树\n",
    "                为了避免每次都重新计算一遍距离，算法会把距离信息保存在一棵树中，这样在计算之前哦操你个梳理查询距离信息，尽量避免重新计算\n",
    "                其基本原理：如果A和B距离很远，B和C距离很近，那么A和C的距离也很远\n",
    "                          有了这个信息，就可以再合适的时候跳过距离远的点\n",
    "                这样优化后的算法复杂度可以降到O（DNiog（N）），\n",
    "                可以参考文献Bentiey，J.L，Communications of the ACM（1975）\n",
    "        "
   ],
   "id": "1a887a1431f708b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
