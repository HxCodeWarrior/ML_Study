{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "算法优化",
   "id": "b6bf4bf13cfc7118"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    学习目标\n",
    "    （1）知道k-means算法的优缺点\n",
    "    （2）知道canopy、K-means++、二分K-means、K-medoids的优化原理\n",
    "    （3）了解kernel K-means、ISODATA、Mini-batch K-means的优化原理"
   ],
   "id": "e9cd6ce17bef2121"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "k-means算法小结",
   "id": "a25c9bcc66367a87"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    优点：\n",
    "        1.原理简单（靠近中心点），实现容易\n",
    "        2.聚类效果中上（依赖K的选择）\n",
    "        3.空间复杂度o(N)，时间复杂度o(IKN)\n",
    "    N为样本点个数，K为中心点个数，I为迭代次数\n",
    "    缺点：\n",
    "        1.对离群点，噪声敏感 （中心点易偏移）\n",
    "        2.很难发现大小差别很大的簇及进行增量计算\n",
    "        3.结果不一定是全局最优，只能保证局部最优（与K的个数及初值选取有关）"
   ],
   "id": "c6384022f29eb56d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1.Canpy算法配合初始聚类",
   "id": "31c8409ea3eb678d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    1.1 Canopy算法配合初始聚类实现流程\n",
    "![jupyter](../Sources/Pictures/Clustering_Algorithms/Canopy算法配合初始聚类实现流程.png)"
   ],
   "id": "dcfe664d837671d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    1.2 Canopy算法的优缺点\n",
    "        优点：\n",
    "            1.Kmeans对噪声抗干扰较弱，通过Canopy对比，将较小的NumPoint的\n",
    "            Cluster直接去掉有利于抗干扰。\n",
    "            2.Canopy选择出来的每个Canopy的centerPoint作为K会更精确。\n",
    "            3.只是针对每个Canopy的内做Kmeans聚类，减少相似计算的数量。\n",
    "        缺点：\n",
    "            1.算法中 T1、T2的确定问题 ，依旧可能落入局部最优解"
   ],
   "id": "fa62302b2f29c298"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2.K-means++",
   "id": "9cdba028fef53923"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![jupyter](../Sources/Pictures/Clustering_Algorithms/K-means++.png)",
   "id": "a8b4f7bb9e161f28"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    其中：\n",
    "![jupyter](../Sources/Pictures/Clustering_Algorithms/K-means++算法推导.png)"
   ],
   "id": "23cd9e7765ec4b33"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    为了方便后面表示，把其记为A\n",
    "![jupyter](../Sources/Pictures/Clustering_Algorithms/K-means++算法推导-1.png)"
   ],
   "id": "d58f95bcf08702c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![jupyter](../Sources/Pictures/Clustering_Algorithms/K-means++算法推导-2.png)",
   "id": "ff27e3035b61de59"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![jupyter](../Sources/Pictures/Clustering_Algorithms/K-means++算法推导-3.png)",
   "id": "99656df37660dbef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    kmeans++目的，让选择的质心尽可能的分散\n",
    "    如下图中，如果第一个质心选择在圆心，那么最优可能选择到的下一个点在P(A)这个区域（\n",
    "    根据颜色进行划分）\n",
    "![jupyter](../Sources/Pictures/Clustering_Algorithms/K-means++目的图解.png)"
   ],
   "id": "2350bb310468664d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3.二分K-means",
   "id": "da67e0828c1721a0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    实现流程:\n",
    "        1.所有点作为一个簇\n",
    "        2.将该簇一分为二\n",
    "        3.选择能最大限度降低聚类代价函数（也就是误差平方和）的簇划分为两个簇。\n",
    "        4.以此进行下去，直到簇的数目等于用户给定的数目k为止。\n",
    "![jupyter](../Sources/Pictures/Clustering_Algorithms/Half_Of_Kmeans图解.png)"
   ],
   "id": "dcda59c8d6915cec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    隐含的一个原则\n",
    "    因为聚类的误差平方和能够衡量聚类性能，该值越小表示数据点越接近于他们的质心，聚类效\n",
    "    果就越好。所以需要对误差平方和最大的簇进行再一次划分，因为误差平方和越大，表示该簇\n",
    "    聚类效果越不好，越有可能是多个簇被当成了一个簇，所以我们首先需要对这个簇进行划分。\n",
    "    \n",
    "    二分K均值算法可以加速K-means算法的执行速度，因为它的相似度计算少了并且不受初始化\n",
    "    问题的影响，因为这里不存在随机点的选取，且每一步都保证了误差最小"
   ],
   "id": "46bd3ab0671229f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4.K-medoids(k-中心聚类算法)",
   "id": "cd06d89b9607b9a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    K-medoids和K-means是有区别的，不一样的地方在于中心点的选取\n",
    "        K-means中，将中心点取为当前cluster中所有数据点的平均值，对异常点很敏感!\n",
    "        K-medoids中，将从当前cluster 中选取到其他所有（当前cluster中的）点的距\n",
    "        离之和最小的点作为中心点。"
   ],
   "id": "b198e8ac284c6be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    算法流程：\n",
    "    (1)总体n个样本点中任意选取k个点作为medoids\n",
    "    (2)按照与medoids最近的原则，将剩余的n-k个点分配到当前最佳的medoids代表的类中\n",
    "    (3)对于第i个类中除对应medoids点外的所有其他点，按顺序计算当其为新的medoids时，\n",
    "       代价函数的值，遍历所有可能，选取代价函数最小时对应的点作为新的medoids\n",
    "    (4)重复2-3的过程，直到所有的medoids点不再发生变化或已达到设定的最大迭代次数\n",
    "    (5)产出最终确定的k个类"
   ],
   "id": "e9b13fbee1eef6ed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    k-medoids对噪声鲁棒性好。\n",
    "    例：当一个cluster样本点只有少数几个，如（1,1）（1,2）（2,1）（1000,1000）。\n",
    "    其中（1000,1000）是噪声。如果按照k-means质心大致会处在（1,1）（1000,1000）\n",
    "    中间，这显然不是我们想要的。这时k-medoids就可以避免这种情况，他会在（1,1）（\n",
    "    1,2）（2,1）（1000,1000）中选出一个样本点使cluster的绝对误差最小，计算可知一\n",
    "    定会在前三个点中选取。\n",
    "\n",
    "    k-medoids只能对小样本起作用，样本大，速度就太慢了，当样本多的时候，少数几个噪音\n",
    "    对k-means的质心影响也没有想象中的那么重，所以k-means的应用明显比k-medoids多。"
   ],
   "id": "3fef7032df27ed76"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5.Kernel k-means(了解)",
   "id": "f0f7b63b0ebd960f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    kernel k-means实际上，就是将每个样本进行一个投射到高维空间的处理，然后再将处理\n",
    "    后的数据使用普通的k-means算法思想进行聚类。\n",
    "![jupyter](../Sources/Pictures/Clustering_Algorithms/Kernel_kmeans算法.png)"
   ],
   "id": "12d98fdbd124a5f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 6.ISODATA（了解）",
   "id": "db19040790428880"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    类别数目随着聚类过程而变化；\n",
    "    对类别数会进行合并，分裂，\n",
    "    “合并”：（当聚类结果某一类中样本数太少，或两个类间的距离太近时）\n",
    "    “分裂”：（当聚类结果中某一类的类内方差太大，将该类进行分裂）"
   ],
   "id": "7009539556760cca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#  7.Mini Batch K-Means（了解）",
   "id": "f7bdf72ffac5de46"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    适合大数据的聚类算法\n",
    "    大数据量是什么量级？通常当样本量大于1万做聚类时，就需要考虑选用Mini Batch K-Means算法。\n",
    "\n",
    "    Mini Batch KMeans使用了Mini Batch（分批处理）的方法对数据点之间的距离进行计\n",
    "    算。Mini Batch计算过程中不必使用所有的数据样本，而是从不同类别的样本中抽取一部分\n",
    "    样本来代表各自类型进行计算。由于计算样本量少，所以会相应的减少运行时间，但另一方面\n",
    "    抽样也必然会带来准确度的下降。\n",
    "    \n",
    "    该算法的迭代步骤有两步：\n",
    "        (1)从数据集中随机抽取一些数据形成小批量，把他们分配给最近的质心\n",
    "        (2)更新质心\n",
    "        \n",
    "    与Kmeans相比，数据的更新在每一个小的样本集上。对于每一个小批量，通过计算平均值得\n",
    "    到更新质心，并把小批量里的数据分配给该质心，随着迭代次数的增加，这些质心的变化是逐\n",
    "    渐减小的，直到质心稳定或者达到指定的迭代次数，停止计算。"
   ],
   "id": "e7542b3f336dedf6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "8.小结",
   "id": "5fea594301945a8f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    k-means算法优缺点总结【知道】\n",
    "    优点：\n",
    "        1.原理简单（靠近中心点），实现容易\n",
    "        2.聚类效果中上（依赖K的选择）\n",
    "        3.空间复杂度o(N)，时间复杂度o(IKN)\n",
    "    缺点：\n",
    "        1.对离群点，噪声敏感 （中心点易偏移）\n",
    "        2.很难发现大小差别很大的簇及进行增量计算\n",
    "        3.结果不一定是全局最优，只能保证局部最优（与K的个数及初值选取有关）"
   ],
   "id": "c14f7419407f65fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    优化方法【知道】",
   "id": "598e49fe4abb71a8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
